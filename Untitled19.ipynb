{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317f7dd0-ed06-4f31-b97d-658ccb8be063",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[0;32m     12\u001b[0m sales \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Load the datasets\n",
    "sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "sales.name = 'sales'\n",
    "calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n",
    "calendar.name = 'calendar'\n",
    "prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "prices.name = 'prices'\n",
    "\n",
    "# Add zero sales for the remaining days 1942-1969\n",
    "for d in range(1942, 1970):\n",
    "    col = 'd_' + str(d)\n",
    "    sales[col] = 0\n",
    "    sales[col] = sales[col].astype(np.int16)  # Downcast to save memory\n",
    "\n",
    "# Downcast function to optimize memory usage\n",
    "def downcast(df):\n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    for i, t in enumerate(types):\n",
    "        if 'int' in str(t):\n",
    "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
    "        elif 'float' in str(t):\n",
    "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
    "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
    "        elif t == np.object:\n",
    "            if cols[i] == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype('category')\n",
    "    return df\n",
    "\n",
    "# Downcast the dataframes\n",
    "sales = downcast(sales)\n",
    "prices = downcast(prices)\n",
    "calendar = downcast(calendar)\n",
    "\n",
    "# Reshape the data for analysis\n",
    "df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()\n",
    "df = pd.merge(df, calendar, on='d', how='left')\n",
    "df = pd.merge(df, prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "\n",
    "# Store the categories along with their codes\n",
    "d_id = dict(zip(df.id.cat.codes, df.id))\n",
    "d_item_id = dict(zip(df.item_id.cat.codes, df.item_id))\n",
    "d_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))\n",
    "d_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))\n",
    "d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))\n",
    "d_state_id = dict(zip(df.state_id.cat.codes, df.state_id))\n",
    "\n",
    "# Convert days to integers\n",
    "df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\n",
    "\n",
    "# Convert categorical columns to codes\n",
    "cols = df.dtypes.index.tolist()\n",
    "types = df.dtypes.values.tolist()\n",
    "for i, type in enumerate(types):\n",
    "    if type.name == 'category':\n",
    "        df[cols[i]] = df[cols[i]].cat.codes\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# Introduce lag features\n",
    "lags = [1, 2, 3, 6, 12, 24, 36]\n",
    "for lag in lags:\n",
    "    df['sold_lag_' + str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], as_index=False)['sold'].shift(lag).astype(np.float16)\n",
    "\n",
    "# Add average sales features\n",
    "df['iteam_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_dept_sold_avg'] = df.groupby(['cat_id', 'dept_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['store_item_sold_avg'] = df.groupby(['store_id', 'item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_item_sold_avg'] = df.groupby(['cat_id', 'item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['dept_item_sold_avg'] = df.groupby(['dept_id', 'item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['state_store_sold_avg'] = df.groupby(['state_id', 'store_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['state_store_cat_sold_avg'] = df.groupby(['state_id', 'store_id', 'cat_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['store_cat_dept_sold_avg'] = df.groupby(['store_id', 'cat_id', 'dept_id'])['sold'].transform('mean').astype(np.float16)\n",
    "\n",
    "# Add rolling and expanding sales features\n",
    "df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)\n",
    "df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)\n",
    "\n",
    "# Add daily average sales and trend features\n",
    "df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd'])['sold'].transform('mean').astype(np.float16)\n",
    "df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(['daily_avg_sold', 'avg_sold'], axis=1, inplace=True)\n",
    "df = df[df['d'] >= 36]\n",
    "\n",
    "# Save memory by downcasting\n",
    "df = downcast(df)\n",
    "\n",
    "# Save data to pickle\n",
    "df.to_pickle('data.pkl')\n",
    "\n",
    "# Perform garbage collection\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# Load data from pickle\n",
    "data = pd.read_pickle('data.pkl')\n",
    "\n",
    "# Split the data for validation and test\n",
    "valid = data[(data['d'] >= 1914) & (data['d'] < 1942)][['id', 'd', 'sold']]\n",
    "test = data[data['d'] >= 1942][['id', 'd', 'sold']]\n",
    "eval_preds = test['sold']\n",
    "valid_preds = valid['sold']\n",
    "\n",
    "# Get the store ids\n",
    "stores = sales.store_id.cat.codes.unique().tolist()\n",
    "for store in stores:\n",
    "    df = data[data['store_id'] == store]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, y_train = df[df['d'] < 1914].drop('sold', axis=1), df[df['d'] < 1914]['sold']\n",
    "    X_valid, y_valid = df[(df['d'] >= 1914) & (df['d'] < 1942)].drop('sold', axis=1), df[(df['d'] >= 1914) & (df['d'] < 1942)]['sold']\n",
    "    X_test = df[df['d'] >= 1942].drop('sold', axis=1)\n",
    "\n",
    "    # Train and validate the model\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        max_depth=8,\n",
    "        num_leaves=50,\n",
    "        min_child_weight=300\n",
    "    )\n",
    "    print('*****Prediction for Store: {}*****'.format(d_store_id[store]))\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse', verbose=20, early_stopping_rounds=20)\n",
    "    \n",
    "    valid_preds[X_valid.index] = model.predict(X_valid)\n",
    "    eval_preds[X_test.index] = model.predict(X_test)\n",
    "    \n",
    "    # Save the model\n",
    "    filename = 'model' + str(d_store_id[store]) + '.pkl'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, X_train, y_train, X_valid, y_valid\n",
    "    gc.collect()\n",
    "\n",
    "# Feature importance plotting\n",
    "feature_importance_df = pd.DataFrame()\n",
    "features = [f for f in data.columns if f != 'sold']\n",
    "for filename in os.listdir('/kaggle/working/'):\n",
    "    if 'model' in filename:\n",
    "        model = joblib.load(filename)\n",
    "        store_importance_df = pd.DataFrame()\n",
    "        store_importance_df[\"feature\"] = features\n",
    "        store_importance_df[\"importance\"] = model.feature_importances_\n",
    "        store_importance_df[\"store\"] = filename[5:9]\n",
    "        feature_importance_df = pd.concat([feature_importance_df, store_importance_df], axis=0)\n",
    "\n",
    "# Display the feature importances\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:20].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (averaged over store predictions)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "display_importances(feature_importance_df)\n",
    "\n",
    "# Prepare the submission file\n",
    "actual = False\n",
    "if actual == False:\n",
    "    # Get the validation results\n",
    "    validation = sales[['id'] + ['d_' + str(i) for i in range(1914, 1942)]]\n",
    "    validation['id'] = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv').id\n",
    "    validation.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "else:\n",
    "    valid['sold'] = valid_preds\n",
    "    validation = valid[['id', 'd', 'sold']]\n",
    "    validation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index()\n",
    "    validation.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "    validation.id = validation.id.map(d_id).str.replace('evaluation', 'validation')\n",
    "\n",
    "# Get the evaluation results\n",
    "test['sold'] = eval_preds\n",
    "evaluation = test[['id', 'd', 'sold']]\n",
    "evaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()\n",
    "evaluation.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "evaluation.id = evaluation.id.map(d_id)\n",
    "\n",
    "# Prepare the final submission\n",
    "submit = pd.concat([validation, evaluation]).reset_index(drop=True)\n",
    "submit.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550562e-2631-4440-8dcd-8825e3ba20b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
